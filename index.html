<!doctype html>
<html><head><title data-react-helmet="true">Casey Chu</title><meta data-react-helmet="true" name="viewport" content="width=device-width, initial-scale=1"/><link data-react-helmet="true" rel="stylesheet" href="/static/style.css"/><link data-react-helmet="true" rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"/></head><body id="index"><div class="index"><header class="header-out"><a href="/" class="avatar"></a><div class="header-info"><div class="header-name">Casey Chu</div><div class="header-email"><a href="mailto:caseychu9@gmail.com">caseychu9@gmail.com</a></div></div></header><header class="index-header"><span class="avatar"></span><div class="index-header-info"><div class="index-header-name">Casey Chu</div><div class="index-header-email"><a href="mailto:caseychu9@gmail.com" target="_blank">caseychu9@gmail.com</a></div></div><div class="index-header-description"><p>Hello! I’m a researcher at <a href="https://openai.com/">OpenAI</a>, currently working on multimodal AI systems. Let’s chat sometime!</p><p>Previously, I was a grad student in <a href="https://icme.stanford.edu">computational math at Stanford University</a> and an undergrad majoring in math at Harvey Mudd College. Find me on <a href="https://twitter.com/caseychu9" rel="me"><i class="fa fa-twitter"></i> Twitter</a>, <a href="http://stackoverflow.com/users/298233/casey" rel="me"><i class="fa fa-stack-overflow"></i> Stack Overflow</a>, and <a href="https://github.com/caseychu" rel="me"><i class="fa fa-github"></i> GitHub</a>, or check out my <a href="/static/resume.pdf" rel="me">resume</a>.</p></div></header><section class="section" id="posts"><h1 class="section-title">Blog Posts</h1><div class="story-basic"><a href="/posts/a-bayesian-trains-a-classifier/" class="story-title">A Bayesian trains a classifier</a><div class="story-date">November 2018</div></div><div class="story-basic"><a href="/posts/perspectives-on-the-variational-autoencoder/" class="story-title">Perspectives on the variational autoencoder</a><div class="story-date">November 2018</div></div><div class="story-basic"><a href="/posts/maximum-entropy-kl-divergence-and-bayesian-inference/" class="story-title">The principle of maximum entropy</a><div class="story-date">July 2018</div></div><div class="story-basic"><a href="/posts/flavors-of-wasserstein-gan/" class="story-title">Flavors of Wasserstein GAN</a><div class="story-date">March 2018</div></div><div class="story-basic"><a href="/posts/why-does-algebra-work/" class="story-title">Why does algebra work?</a><div class="story-date">January 2013</div></div><div class="story-basic"><a href="/posts/the-dirichlet-function-in-closed-form/" class="story-title">The Dirichlet function in closed form</a><div class="story-date">March 2012</div></div></section><section class="section" id="publications"><h1 class="section-title">Research</h1><div class="story story-publication "><a href="https://arxiv.org/abs/2004.01822" class="story-img" style="background-image:url(/static/content/svgd.png)" target="_blank"></a><div class="story-info"><a href="https://arxiv.org/abs/2004.01822" class="story-title" target="_blank">The equivalence between Stein variational gradient descent and black-box variational inference</a><div class="story-date">2020</div><div class="story-authors"><b>Casey Chu</b>, Kentaro Minami, Kenji Fukumizu</div><div class="story-venue">ICLR 2020 <a href="http://iclr2020deepdiffeq.rice.edu/">DeepDiffEq Workshop</a></div><div class="story-blurb">We formalize an equivalence between two popular methods for Bayesian inference: Stein variational gradient descent (SVGD) and black-box variational inference (BBVI). In particular, we show that BBVI corresponds precisely to SVGD when the kernel is the neural tangent kernel. Furthermore, we interpret SVGD and BBVI as kernel gradient flows; we do this by leveraging the recent perspective that views SVGD as a gradient flow in the space of probability distributions and showing that BBVI naturally motivates a Riemannian structure on that space. We observe that kernel gradient flow also describes dynamics found in the training of generative adversarial networks (GANs). This work thereby unifies several existing techniques in variational inference and generative modeling and identifies the kernel as a fundamental object governing the behavior of these algorithms, motivating deeper analysis of its properties.</div></div></div><div class="story story-publication "><a href="https://arxiv.org/abs/2002.04185" class="story-img" style="background-image:url(/static/content/smoothness.png)" target="_blank"></a><div class="story-info"><a href="https://arxiv.org/abs/2002.04185" class="story-title" target="_blank">Smoothness and Stability in GANs</a><div class="story-date">2020</div><div class="story-authors"><b>Casey Chu</b>, Kentaro Minami, Kenji Fukumizu</div><div class="story-venue">ICLR 2020</div><div class="story-blurb">Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.</div></div></div><div class="story story-publication "><a href="https://arxiv.org/abs/1901.10691" class="story-img" style="background-image:url(/static/content/influence.png)" target="_blank"></a><div class="story-info"><a href="https://arxiv.org/abs/1901.10691" class="story-title" target="_blank">Probability Functional Descent: A Unifying Perspective on GANs, Variational Inference, and Reinforcement Learning</a><div class="story-date">2019</div><div class="story-authors"><b>Casey Chu</b>, Jose Blanchet, Peter Glynn</div><div class="story-venue">ICML 2019</div><div class="story-blurb">This paper provides a unifying view of a wide range of problems of interest in machine learning by framing them as the minimization of functionals defined on the space of probability measures. In particular, we show that generative adversarial networks, variational inference, and actor-critic methods in reinforcement learning can all be seen through the lens of our framework. We then discuss a generic optimization algorithm for our formulation, called probability functional descent (PFD), and show how this algorithm recovers existing methods developed independently in the settings mentioned earlier.</div></div></div><div class="story story-publication "><a href="https://arxiv.org/abs/1712.02950" class="story-img" style="background-image:url(/static/content/cyclegan.png)" target="_blank"></a><div class="story-info"><a href="https://arxiv.org/abs/1712.02950" class="story-title" target="_blank">CycleGAN, a Master of Steganography</a><div class="story-date">2017</div><div class="story-authors"><b>Casey Chu</b>, Andrey Zhmoginov, Mark Sandler</div><div class="story-venue">NIPS 2017 <a href="https://www.machinedeception.com/" target="_blank">Workshop on Machine Deception</a></div><div class="story-blurb">CycleGAN <a href="https://junyanz.github.io/CycleGAN/" target="_blank">(Zhu et al. 2017)</a> is one recent successful approach to learn a transformation between two image distributions. In a series of experiments, we demonstrate an intriguing property of the model: CycleGAN learns to "hide" information about a source image into the images it generates in a nearly imperceptible, high-frequency signal. This trick ensures that the generator can recover the original sample and thus satisfy the cyclic consistency requirement, while the generated image remains realistic. We connect this phenomenon with adversarial attacks by viewing CycleGAN's training procedure as training a generator of adversarial examples and demonstrate that the cyclic consistency loss causes CycleGAN to be especially vulnerable to adversarial attacks.</div></div></div></section><section class="section" id="notes"><h1 class="section-title">Course Notes</h1><div class="story-basic"><a href="/notes/algebraic-topology/" class="story-title">Algebraic topology</a></div><div class="story-basic"><a href="/notes/algorithms/" class="story-title">Algorithms</a></div><div class="story-basic"><a href="/notes/numerical-linear-algebra/" class="story-title">Numerical linear algebra</a></div><div class="story-basic"><a href="/notes/numerical-partial-differential-equations/" class="story-title">Numerical partial differential equations</a></div><div class="story-basic"><a href="/notes/partial-differential-equations/" class="story-title">Partial differential equations</a></div><div class="story-basic"><a href="/notes/statistical-mechanics/" class="story-title">Statistical mechanics</a></div><div class="story-basic"><a href="/notes/reinforcement-learning/" class="story-title">Reinforcement learning</a></div></section></div></body></html>