<!doctype html>
<html><head><title data-react-helmet="true">Casey Chu</title><meta data-react-helmet="true" name="viewport" content="width=device-width, initial-scale=1"/><link data-react-helmet="true" rel="stylesheet" href="/static/style.css"/><link data-react-helmet="true" rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"/></head><body id="index"><div class="index"><header class="header-out"><a href="/" class="avatar"></a><div class="header-info"><div class="header-name">Casey Chu</div><div class="header-email"><a href="mailto:caseychu@stanford.edu">caseychu@stanford.edu</a></div></div></header><header class="index-header"><span class="avatar"></span><div class="index-header-info"><div class="index-header-name">Casey Chu</div><div class="index-header-email"><a href="mailto:caseychu@stanford.edu" target="_blank">caseychu@stanford.edu</a></div></div><div class="index-header-description"><p>Hello! Iâ€™m a Ph.D. student in <a href="https://icme.stanford.edu">computational math</a> at Stanford University, specializing in deep learning.</p><p>Previously, I majored in math at Harvey Mudd College, and I was an intern at Google and Facebook. Find me on <a href="https://twitter.com/caseychu9" rel="me"><i class="fa fa-twitter"></i> Twitter</a>, <a href="http://stackoverflow.com/users/298233/casey" rel="me"><i class="fa fa-stack-overflow"></i> Stack Overflow</a>, and <a href="https://github.com/caseychu" rel="me"><i class="fa fa-github"></i> GitHub</a>, or check out my <a href="/static/resume.pdf" rel="me">resume</a>.</p></div></header><section class="section" id="posts"><h1 class="section-title">Blog Posts</h1><div class="story-basic"><a href="/posts/a-bayesian-trains-a-classifier/" class="story-title">A Bayesian trains a classifier</a><div class="story-date">November 2018</div></div><div class="story-basic"><a href="/posts/perspectives-on-the-variational-autoencoder/" class="story-title">Perspectives on the variational autoencoder</a><div class="story-date">November 2018</div></div><div class="story-basic"><a href="/posts/maximum-entropy-kl-divergence-and-bayesian-inference/" class="story-title">Maximum entropy, KL divergence, and Bayesian inference</a><div class="story-date">July 2018</div></div><div class="story-basic"><a href="/posts/flavors-of-wasserstein-gan/" class="story-title">Flavors of Wasserstein GAN</a><div class="story-date">March 2018</div></div><div class="story-basic"><a href="/posts/why-does-algebra-work/" class="story-title">Why does algebra work?</a><div class="story-date">January 2013</div></div><div class="story-basic"><a href="/posts/the-dirichlet-function-in-closed-form/" class="story-title">The Dirichlet function in closed form</a><div class="story-date">March 2012</div></div></section><section class="section" id="publications"><h1 class="section-title">Research</h1><div class="story story-publication "><a href="https://arxiv.org/abs/1901.10691" class="story-img" style="background-image:url(/static/content/influence.png)" target="_blank"></a><div class="story-info"><a href="https://arxiv.org/abs/1901.10691" class="story-title" target="_blank">Probability Functional Descent: A Unifying Perspective on GANs, Variational Inference, and Reinforcement Learning</a><div class="story-date">2019</div><div class="story-authors"><b>Casey Chu</b>, Jose Blanchet, Peter Glynn</div><div class="story-venue"></div><div class="story-blurb">This paper provides a unifying view of a wide range of problems of interest in machine learning by framing them as the minimization of functionals defined on the space of probability measures. In particular, we show that generative adversarial networks, variational inference, and actor-critic methods in reinforcement learning can all be seen through the lens of our framework. We then discuss a generic optimization algorithm for our formulation, called probability functional descent (PFD), and show how this algorithm recovers existing methods developed independently in the settings mentioned earlier.</div></div></div><div class="story story-publication "><a href="https://arxiv.org/abs/1712.02950" class="story-img" style="background-image:url(/static/content/cyclegan.png)" target="_blank"></a><div class="story-info"><a href="https://arxiv.org/abs/1712.02950" class="story-title" target="_blank">CycleGAN, a Master of Steganography</a><div class="story-date">2017</div><div class="story-authors"><b>Casey Chu</b>, Andrey Zhmoginov, Mark Sandler</div><div class="story-venue">NIPS 2017 <a href="https://www.machinedeception.com/" target="_blank">Workshop on Machine Deception</a></div><div class="story-blurb">CycleGAN <a href="https://junyanz.github.io/CycleGAN/" target="_blank">(Zhu et al. 2017)</a> is one recent successful approach to learn a transformation between two image distributions. In a series of experiments, we demonstrate an intriguing property of the model: CycleGAN learns to "hide" information about a source image into the images it generates in a nearly imperceptible, high-frequency signal. This trick ensures that the generator can recover the original sample and thus satisfy the cyclic consistency requirement, while the generated image remains realistic. We connect this phenomenon with adversarial attacks by viewing CycleGAN's training procedure as training a generator of adversarial examples and demonstrate that the cyclic consistency loss causes CycleGAN to be especially vulnerable to adversarial attacks.</div></div></div></section><section class="section" id="notes"><h1 class="section-title">Course Notes</h1><div class="story-basic"><a href="/notes/algebraic-topology/" class="story-title">Algebraic topology</a></div><div class="story-basic"><a href="/notes/algorithms/" class="story-title">Algorithms</a></div><div class="story-basic"><a href="/notes/numerical-linear-algebra/" class="story-title">Numerical linear algebra</a></div><div class="story-basic"><a href="/notes/numerical-partial-differential-equations/" class="story-title">Numerical partial differential equations</a></div><div class="story-basic"><a href="/notes/partial-differential-equations/" class="story-title">Partial differential equations</a></div><div class="story-basic"><a href="/notes/statistical-mechanics/" class="story-title">Statistical mechanics</a></div><div class="story-basic"><a href="/notes/reinforcement-learning/" class="story-title">Reinforcement learning</a></div></section></div></body></html>